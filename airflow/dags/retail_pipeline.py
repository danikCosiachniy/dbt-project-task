from __future__ import annotations

import os
from datetime import datetime
from typing import Any, Literal, cast

from cosmos import DbtTaskGroup, ExecutionConfig, ProfileConfig, ProjectConfig

from airflow import DAG
from airflow.models import Variable
from utils.constants import DBT_PROJECT_PATH, VAR_NAME
from utils.dbt_logger import log_failure_callback, log_start_callback, log_success_callback

# Allowed dbt targets configured in profiles.yml
DbtTarget = Literal['snowflake', 'dev']


def get_dbt_target_from_env() -> DbtTarget:
    """
    Read dbt target from the DBT_TARGET env var.

    Valid values:
      - "snowflake" -> run against Snowflake
      - "dev"       -> run against DuckDB

    Defaults to "snowflake" if not set or invalid.
    """
    value = os.getenv('DBT_TARGET', 'snowflake').lower()
    if value not in ('snowflake', 'dev'):
        return 'snowflake'
    return cast(DbtTarget, value)


def get_env(target: DbtTarget) -> dict[str, str]:
    """
    Build env dict for dbt run, depending on target.

    - dev (DuckDB): no extra env needed â†’ {}
    - snowflake: read creds from Airflow Variable <var_name> (JSON) and map to SNOWFLAKE_*.
    """
    if target == 'dev':
        # DuckDB uses a local file path only (no credentials)
        return {}
    if target == 'snowflake':
        try:
            config = Variable.get(VAR_NAME, deserialize_json=True)

            return {
                'SNOWFLAKE_ACCOUNT': config.get('account'),
                'SNOWFLAKE_USER': config.get('user'),
                'SNOWFLAKE_PASSWORD': config.get('password'),  # nosec
                'SNOWFLAKE_ROLE': config.get('role'),
                'SNOWFLAKE_WAREHOUSE': config.get('warehouse'),
                'SNOWFLAKE_DATABASE': config.get('database'),
                'SNOWFLAKE_SCHEMA': config.get('schema'),
            }
        except (KeyError, ValueError) as e:
            print(f'Warning: Could not fetch variable {VAR_NAME}: {e}')
            return {}
    return {}


# Resolve which dbt target to use (Snowflake or DuckDB) from env
DBT_TARGET: DbtTarget = get_dbt_target_from_env()

# dbt profile configuration (points to profiles.yml inside the project)
profile_config: ProfileConfig = ProfileConfig(
    profile_name='retail_vault',
    target_name=DBT_TARGET,
    profiles_yml_filepath=f'{DBT_PROJECT_PATH}/profiles.yml',
)

# Default arguments applied to all tasks in the DAG
default_args: dict[str, Any] = {
    'owner': 'airflow',
    'on_failure_callback': log_failure_callback,
    'on_success_callback': log_success_callback,
    'on_execute_callback': log_start_callback,
}

# Main Airflow DAG definition
with DAG(
    dag_id='retail_vault_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    default_args=default_args,
    tags=['dbt', 'retail', DBT_TARGET],
) as dag:
    # Fetch credentials before creating tasks
    dbt_env = get_env(DBT_TARGET)
    # Group of dbt tasks generated by Cosmos
    dbt_processing: DbtTaskGroup = DbtTaskGroup(
        group_id='dbt_core_logic',
        project_config=ProjectConfig(DBT_PROJECT_PATH),
        profile_config=profile_config,
        execution_config=ExecutionConfig(
            dbt_executable_path='dbt',
        ),
        operator_args={
            'install_deps': True,  # run `dbt deps` before execution
            'full_refresh': False,  # use incremental logic where applicable
            'env': dbt_env,  # Passing the secure variables here
        },
    )
