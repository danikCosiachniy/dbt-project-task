# Retail Data Vault Pipeline

This repository contains an ETL/ELT pipeline for retail data processing built using the **Data Vault 2.0** methodology.
Transformations are implemented with **dbt**, orchestration is handled by **Apache Airflow** using **Astronomer Cosmos** for native dbt integration.

The project is designed to work with **Snowflake** as the analytical warehouse and follows a layered architecture:
**staging â†’ raw vault â†’ business vault â†’ marts**.

---

## ğŸ›  Tech Stack

- **Orchestration:** Apache Airflow 2.10+ (Cosmos)
- **Transformations:** dbt Core 1.7+
- **Data Warehouse:** Snowflake
- **Metadata & Orchestration DB:** Postgres (Airflow)
- **Dependency Management:** uv (Astral)
- **Infrastructure:** Docker & Docker Compose
- **Methodology:** Data Vault 2.0

---

## ğŸ” Credentials & Secrets Management

âš ï¸ **Important:** Credentials are **not stored in `.env` files**.

### Telegram notifications
Telegram credentials are stored in **Airflow Variables** as a single JSON object:

**Variable name:** `telegram_credentials`

```json
{
  "bot_token": "<BOT_TOKEN>",
  "chat_id": "<CHAT_ID>"
}
```
# ğŸ” Credentials & Configuration

These credentials are used by Airflow callbacks and utilities for system notifications.

## â„ï¸ Snowflake Credentials

Snowflake credentials must be stored in **Airflow Connections** to ensure secure access.

* **Connection ID:** `snowflake_default`
* **Connection Type:** `Snowflake`

**Required Fields:**
* `Account`
* `User`
* `Password` / `Key`
* `Role`
* `Warehouse`
* `Database`
* `Schema`

> **Note:** Both **dbt** and **Airflow** rely on this connection via Cosmos and custom dbt runners.

---

# ğŸš€ Quick Start

Follow these steps to get the project running locally.

### 1. Environment Configuration (`.env`)

Create a `.env` file in the root directory of the project.

**Important:** This file is used **only** for Airflow and Docker infrastructure configuration. It is **not** used for warehouse credentials.

```bash
# Example command to create .env
touch .env
```

```ini
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=./airflow

AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow

PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python

AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User
AIRFLOW_ADMIN_EMAIL=admin@example.com
```
### 2. Build & Run ğŸ› ï¸

The project is managed via **Makefile** (recommended approach).

**Full Rebuild**
Run this for the first setup or after changing dependencies in `pyproject.toml`

```bash
make rebuild
```
Equivalent manual command:
```bash
docker compose up -d --build --force-recreate
```
Start existing containers
```bash
make up
```
### 3. Access ğŸ–¥ï¸

* **Airflow UI:** [http://localhost:8080](http://localhost:8080)
* **Login / Password:** Sourced from `.env` (Default: `admin` / `admin`)

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ airflow/                         # Airflow-specific code and configuration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ retail_pipeline.py       # Main DAG (Cosmos-based dbt orchestration)
â”‚   â”‚   â””â”€â”€ utils/                   # Helpers (dbt runner, notifications, callbacks)
â”‚   â”œâ”€â”€ logs/                        # Airflow logs (mounted volume)
â”‚   â”œâ”€â”€ plugins/                     # Optional custom Airflow plugins
â”‚   â””â”€â”€ README.md                    # Airflow-specific documentation
â”‚
â”œâ”€â”€ dbt_vault_retail/                # dbt project root
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                 # Source-aligned staging models
â”‚   â”‚   â”œâ”€â”€ raw_vault/               # Hubs, Links, Satellites
â”‚   â”‚   â”œâ”€â”€ business_vault/          # PITs, effectivity sats, business sats
â”‚   â”‚   â””â”€â”€ marts/                   # Dimensions and facts
â”‚   â”œâ”€â”€ macros/                      # Shared dbt macros
â”‚   â”œâ”€â”€ seeds/                       # Seed data (e.g. customer_master)
â”‚   â”œâ”€â”€ snapshots/                   # dbt snapshots (optional)
â”‚   â”œâ”€â”€ profiles.yml                 # dbt profile (uses Airflow connection)
â”‚   â”œâ”€â”€ dbt_project.yml              # dbt project configuration
â”‚   â””â”€â”€ README.md                    # dbt-specific documentation
â”‚
â”œâ”€â”€ docker-compose.yaml              # Docker services (Airflow, Postgres)
â”œâ”€â”€ Dockerfile                       # Custom Airflow image with dbt & Cosmos
â”œâ”€â”€ Makefile                         # Project commands (build, lint, dbt runs)
â”œâ”€â”€ pyproject.toml                   # Python dependencies (uv / PEP 621)
â”œâ”€â”€ requirements.txt                 # Exported deps for Docker build
â”œâ”€â”€ uv.lock                          # Dependency lockfile
â””â”€â”€ README.md                        # Root documentation (this file)
```

## â–¶ï¸ Running dbt Pipelines

All dbt commands are executed inside the Airflow container using a custom runner.

**Full load** (seeds + full-refresh build)
```bash
make initial-load
```
**Incremental load**
```bash
make incremental-load
```
## ğŸ§ª Linting & Quality Checks

All linters are executed via pre-commit.

```bash
make lint
```
**This includes:**
* Python linting/formatting (`ruff`)
* SQL linting (`sqlfluff`)
* YAML & whitespace checks

---
## ğŸ“¦ Dependency Management (uv)

Dependencies are managed with `uv`.

**Add a new Python dependency:**

1. Edit `pyproject.toml`
2. Regenerate lockfile and export requirements:
```bash
uv lock
```
3. Rebuild containers:
```bash
make rebuild
```
This guarantees consistent versions across:
* **Airflow**
* **dbt**
* **Cosmos**
* **Local development**

---
## ğŸ“Œ Notes

* **Warehouse credentials** are **never** stored in code or `.env`.
* All dbt models follow **Data Vault 2.0** best practices.
* **Facts and dimensions** are built only from Vault layers, never directly from staging.
* **PIT tables** provide historical â€œas-ofâ€ business views.

---

## ğŸ“ Related Documentation

* `airflow/README.md` â€” Airflow DAGs & orchestration details
* `dbt_vault_retail/README.md` â€” Data Vault & dbt architecture
