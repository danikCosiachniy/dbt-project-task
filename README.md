# Retail Data Vault Pipeline

This project is an ETL/ELT pipeline for retail data processing. It implements **Data Vault 2.0** methodology using **dbt** for transformations and **Apache Airflow** (via Astronomer Cosmos) for orchestration.

## ðŸ›  Tech Stack

- **Orchestration:** Apache Airflow 2.10+
- **Transformation:** dbt Core 1.7+
- **Database:** Postgres / DuckDB (depending on the profile)
- **Dependency Management:** `uv` (Astral)
- **Infrastructure:** Docker & Docker Compose

## ðŸš€ Quick Start

### 1. Environment Configuration (.env)

Create a `.env` file in the root of the project and copy the configuration below. These variables are used in `docker-compose.yaml` to initialize the Airflow admin and configure the database.

**`.env` example:**

```ini
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=./airflow

AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow

PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python

AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=your_password
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User
AIRFLOW_ADMIN_EMAIL=admin@example.com

SNOWFLAKE_ACCOUNT=your_snowflake_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=your_role
SNOWFLAKE_WAREHOUSE=your_wh
SNOWFLAKE_DATABASE=your_db
SNOWFLAKE_SCHEMA=your_schema
```

### 2\. Build & Run

Use **Makefile** or `docker-compose` to build and start all services.

#### ðŸ”¨ Build & Start (recommended for first run or after dependency updates)

```bash
make rebuild
# or manually:
docker-compose up -d --build --force-recreate
```

#### Start (existing containers)

```bash
make up
# or manually:
docker-compose up -d
```

### 3\. Access

  - **Airflow UI:** [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080)
  - **Login/Password:** As defined in your `.env` (default: `admin`/`admin`).

### 4\. Project Structure
```text
.
â”œâ”€â”€ airflow/                         # Airflow-specific project area
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ retail_pipeline.py       # Main Airflow DAG using Cosmos
â”‚   â”‚   â””â”€â”€ utils/                   # Utility modules (logging, callbacks, helpers)
â”‚   â”œâ”€â”€ logs/                        # Airflow logs (mounted volume)
â”‚   â”œâ”€â”€ plugins/                     # (Optional) custom Airflow plugins
â”‚   â””â”€â”€ README.md                    # Docs for Airflow DAGs / plugins
â”‚
â”œâ”€â”€ dbt_project/                     # dbt project root
â”‚   â”œâ”€â”€ models/                      # dbt models (raw, staging, vault, marts)
â”‚   â”œâ”€â”€ macros/                      # dbt macros
â”‚   â”œâ”€â”€ seeds/                       # Input CSVs (e.g., raw_orders.csv)
â”‚   â”œâ”€â”€ snapshots/                   # dbt snapshots
â”‚   â”œâ”€â”€ profiles.yml                 # dbt profile for Snowflake/DuckDB
â”‚   â”œâ”€â”€ dbt_project.yml              # dbt project config
â”‚   â””â”€â”€ README.md                    # Documentation for dbt project
â”‚
â”œâ”€â”€ docker-compose.yaml              # Multi-service orchestration (Airflow + Postgres)
â”œâ”€â”€ Dockerfile                       # Airflow image with dbt & cosmos dependencies
â”œâ”€â”€ Makefile                         # Shortcuts for build/run commands
â”œâ”€â”€ pyproject.toml                   # Python project definition (uv/PEP 621)
â”œâ”€â”€ requirements.txt                 # Exported dependencies for Airflow build
â”œâ”€â”€ uv.lock                          # uv lockfile
â”œâ”€â”€ scripts/                         # Utility scripts (optional)
â””â”€â”€ README.md                        # You are here
```

### 5\. Development & Dependency Workflow

This project uses `uv` for dependency management.

**Adding a new Python library:**

1.  Add the dependency into `pyproject.toml`:

    ```toml
    dependencies = [
        "new-library>=1.0",
        ...
    ]
    ```

2.  Export dependencies for Docker:

    ```bash
    uv lock
    uv export --format requirements.txt --no-dev > requirements.txt
    ```

3.  Rebuild your environment:

    ```bash
    make rebuild
    ```

This ensures Airflow, dbt, Cosmos, and your DAGs all use a consistent dependency set.
