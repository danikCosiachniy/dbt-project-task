from __future__ import annotations

from datetime import datetime
from typing import Any

from cosmos import DbtTaskGroup, ExecutionConfig, ProfileConfig, ProjectConfig

from airflow import DAG
from airflow.models import Variable
from utils.dbt_logger import log_failure_callback, log_start_callback, log_success_callback


def get_snowflake_env(var_name: str) -> dict[str, str]:
    """
    Fetches JSON config from Airflow Variables and maps it
    to the specific env vars expected by profiles.yml
    """
    try:
        config = Variable.get(var_name, deserialize_json=True)

        return {
            'SNOWFLAKE_ACCOUNT': config.get('account'),
            'SNOWFLAKE_USER': config.get('user'),
            'SNOWFLAKE_PASSWORD': config.get('password'),  # nosec
            'SNOWFLAKE_ROLE': config.get('role'),
            'SNOWFLAKE_WAREHOUSE': config.get('warehouse'),
            'SNOWFLAKE_DATABASE': config.get('database'),
            'SNOWFLAKE_SCHEMA': config.get('schema'),
        }
    except (KeyError, ValueError) as e:
        print(f'Warning: Could not fetch variable {var_name}: {e}')
        return {}


# Absolute path to the mounted dbt project inside the Airflow container
DBT_PROJECT_PATH: str = '/opt/airflow/dbt_project'

# dbt profile configuration (points to profiles.yml inside the project)
profile_config: ProfileConfig = ProfileConfig(
    profile_name='retail_vault_profile',
    target_name='snowflake',
    profiles_yml_filepath=f'{DBT_PROJECT_PATH}/profiles.yml',
)

# Default arguments applied to all tasks in the DAG
default_args: dict[str, Any] = {
    'owner': 'airflow',
    'on_failure_callback': log_failure_callback,
    'on_success_callback': log_success_callback,
    'on_execute_callback': log_start_callback,
}

# Main Airflow DAG definition
with DAG(
    dag_id='retail_vault_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    default_args=default_args,
    tags=['dbt', 'retail', 'snowflake'],
) as dag:
    # Fetch credentials before creating tasks
    snowflake_env = get_snowflake_env('dbt_snowflake_secrets')
    # Group of dbt tasks generated by Cosmos
    dbt_processing: DbtTaskGroup = DbtTaskGroup(
        group_id='dbt_core_logic',
        project_config=ProjectConfig(DBT_PROJECT_PATH),
        profile_config=profile_config,
        execution_config=ExecutionConfig(
            dbt_executable_path='dbt',
        ),
        operator_args={
            'install_deps': True,  # run `dbt deps` before execution
            'full_refresh': False,  # use incremental logic where applicable
            'env': snowflake_env,  # Passing the secure variables here
        },
    )
