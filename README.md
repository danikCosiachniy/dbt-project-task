# Retail Data Vault Pipeline

This repository contains an ETL/ELT pipeline for retail data processing built using the **Data Vault 2.0** methodology.
Transformations are implemented with **dbt**, orchestration is handled by **Apache Airflow** using **Astronomer Cosmos** for native dbt integration.

The project is designed to work with **Snowflake** as the analytical warehouse and follows a layered architecture:
**staging â†’ raw vault â†’ business vault â†’ marts**.

---

## ğŸ›  Tech Stack

- **Orchestration:** Apache Airflow 2.10+ (Cosmos)
- **Transformations:** dbt Core 1.7+
- **Data Warehouse:** Snowflake
- **Metadata & Orchestration DB:** Postgres (runs inside the Airflow container)
- **Dependency Management:** uv (Astral)
- **Infrastructure:** Docker (single-container local setup)
- **Methodology:** Data Vault 2.0

---

## ğŸ” Credentials & Secrets Management

âš ï¸ **Important:** Warehouse credentials are **not stored in `.env`**.

### Telegram notifications

Telegram credentials are stored in **Airflow Variables** as a single JSON object.

**Variable name:** `telegram_credentials`

```json
{
  "bot_token": "<BOT_TOKEN>",
  "chat_id": "<CHAT_ID>"
}
```
### â„ï¸ Snowflake credentials

Snowflake credentials must be stored in **Airflow Connections**.

- **Connection ID:** `snowflake_default`
- **Connection Type:** `Snowflake`

**Required fields:**
- `Account`
- `User`
- `Password` / `Key`
- `Role`
- `Warehouse`
- `Database`
- `Schema`

> Both **dbt** and **Airflow** rely on this connection via Cosmos and custom dbt runners.

---

# ğŸš€ Quick Start

## 1) Environment Configuration (`.env`)

Create a `.env` file in the root directory.

**Important:** This file is used **only** for Airflow/Docker infrastructure configuration. It is **not** used for warehouse credentials.

```bash
touch .env
```

Example `.env`:

```ini
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=./airflow

POSTGRES_USER=airflow
POSTGRES_DB=airflow
POSTGRES_PASSWORD=airflow
PGDATA=/var/lib/postgresql/data
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@127.0.0.1:5432/airflow

PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python

AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User
AIRFLOW_ADMIN_EMAIL=admin@example.com
```

---

## 2) Build & Run ğŸ› ï¸

The project is managed via **Makefile** (recommended approach).

**Build image**
```bash
make build
```

**Run container**
```bash
make up
```

**Full rebuild (rebuild + restart)**
```bash
make rebuild
```

---

## 3) Access ğŸ–¥ï¸

- **Airflow UI:** http://localhost:8080
- **Login / Password:** sourced from `.env` (example: `admin / admin`)

---

## â–¶ï¸ Running dbt Pipelines

All dbt commands are executed **inside the Airflow container** using a custom runner.

**Full load** (deps + seeds + full-refresh build)
```bash
make initial-load
```

**Incremental load**
```bash
make incremental-load
```

**Clean artifacts**
```bash
make clean-up
```

---

## ğŸ§ª Linting & Quality Checks

All linters are executed via pre-commit.

```bash
make lint
```

This includes:
- Python linting/formatting (`ruff`)
- SQL linting (`sqlfluff`)
- YAML & whitespace checks

---

## ğŸ“¦ Dependency Management (uv)

Dependencies are managed with `uv`.

**Add a new Python dependency:**

1. Edit `pyproject.toml`
2. Regenerate lockfile:
   ```bash
   uv lock
   ```
3. Rebuild image:
   ```bash
   make rebuild
   ```

This guarantees consistent versions across:
- Airflow
- dbt
- Cosmos
- Local development

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ airflow/                         # Airflow-specific code and configuration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ retail_pipeline.py       # Main DAG (Cosmos-based dbt orchestration)
â”‚   â”‚   â””â”€â”€ utils/                   # Helpers (dbt runner, notifications, callbacks)
â”‚   â”œâ”€â”€ logs/                        # Airflow logs (mounted)
â”‚   â”œâ”€â”€ plugins/                     # Optional custom Airflow plugins
â”‚   â””â”€â”€ README.md                    # Airflow-specific documentation
â”‚
â”œâ”€â”€ dbt_vault_retail/                # dbt project root
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                 # Source-aligned staging models
â”‚   â”‚   â”œâ”€â”€ raw_vault/               # Hubs, Links, Satellites
â”‚   â”‚   â”œâ”€â”€ business_vault/          # PITs, effectivity sats, business sats
â”‚   â”‚   â””â”€â”€ marts/                   # Dimensions and facts
â”‚   â”œâ”€â”€ macros/                      # Shared dbt macros
â”‚   â”œâ”€â”€ seeds/                       # Seed data (e.g. customer_master)
â”‚   â”œâ”€â”€ snapshots/                   # dbt snapshots (optional)
â”‚   â”œâ”€â”€ profiles.yml                 # dbt profile (uses Airflow connection)
â”‚   â”œâ”€â”€ dbt_project.yml              # dbt project configuration
â”‚   â””â”€â”€ README.md                    # dbt-specific documentation
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ entrypoint.sh                # Starts Postgres + runs Airflow migrations + starts scheduler/webserver
â”œâ”€â”€ Dockerfile                       # Custom Airflow image with dbt & Cosmos deps
â”œâ”€â”€ Makefile                         # Project commands (build, run, lint, dbt runs)
â”œâ”€â”€ pyproject.toml                   # Python dependencies (uv / PEP 621)
â”œâ”€â”€ uv.lock                          # Dependency lockfile
â””â”€â”€ README.md                        # Root documentation (this file)
```

---

## ğŸ“Œ Notes

- Warehouse credentials are **never** stored in code or `.env`.
- All dbt models follow **Data Vault 2.0** best practices.
- Facts and dimensions are built only from Vault layers, never directly from staging.
- PIT tables provide historical â€œas-ofâ€ business views.

---

## ğŸ“ Related Documentation

- `airflow/README.md` â€” Airflow DAGs & orchestration details
- `dbt_vault_retail/README.md` â€” Data Vault & dbt architecture
